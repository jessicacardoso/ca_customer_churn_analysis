{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from textwrap import wrap\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prepare import prepare_data\n",
    "\n",
    "X_train, X_test = prepare_data(\n",
    "    project_id=\"ca-churn-project\",\n",
    "    database_name=\"customer_churn\",\n",
    "    table_name=\"customer_churn_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoric_columns = X_train.select_dtypes(include=[\"category\"]).columns\n",
    "\n",
    "X_train[\"receita_total\"] = X_train[\"receita_total\"].fillna(X_train[\"receita_mensal\"])\n",
    "X_test[\"receita_total\"] = X_test[\"receita_total\"].fillna(X_test[\"receita_mensal\"])\n",
    "\n",
    "y_train = X_train.pop(\"churn\")\n",
    "y_test = X_test.pop(\"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_columns = [\n",
    "    \"tipo_de_empresa\",\n",
    "    \"funcionarios\",\n",
    "    \"_modulo_financeiro\",\n",
    "    \"_emissao_de_nota_fiscal\",\n",
    "    \"_integracao_bancaria\",\n",
    "    \"_modulo_de_vendas\",\n",
    "    \"_relatorios\",\n",
    "    \"_utilizacao_de_apis_de_integracao\",\n",
    "    \"contrato\",\n",
    "    \"frequencia_de_pagamento\",\n",
    "]\n",
    "\n",
    "numeric_columns = [\n",
    "    \"fundacao_da_empresa\",\n",
    "    \"meses_de_permanencia\",\n",
    "    \"receita_mensal\",\n",
    "    \"receita_total\",\n",
    "]\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"ordinal\",\n",
    "            ordinal_encoder,\n",
    "            ordinal_columns,\n",
    "        ),\n",
    "        (\n",
    "            \"one_hot\",\n",
    "            one_hot_encoder,\n",
    "            [\n",
    "                \"faz_conciliacao_bancaria\",\n",
    "                \"forma_de_pagamento\",\n",
    "                \"possuicontador\",\n",
    "                \"emite_boletos\",\n",
    "                \"possui_mais_de_um_socio\",\n",
    "                \"utiliza_servicos_financeiros\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"scaler\",\n",
    "            scaler,\n",
    "            numeric_columns,\n",
    "        ),\n",
    "    ]\n",
    ").set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    clf_name = trial.suggest_categorical(\n",
    "        \"clf_name\",\n",
    "        [\n",
    "            \"LinearSVC\",\n",
    "            \"DecisionTreeClassifier\",\n",
    "            \"RandomForestClassifier\",\n",
    "            \"BalancedRandomForestClassifier\",\n",
    "            \"LogisticRegression\",\n",
    "            \"XGBClassifier\",\n",
    "            \"LGBMClassifier\",\n",
    "            \"DummyClassifier\",\n",
    "            \"IsolationForest\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # resampling = trial.suggest_categorical(\n",
    "    #     \"resampling\", [\"none\", \"SMOTEENN\", \"SMOTETomek\", \"ClusterCentroids\", \"ADASYN\"]\n",
    "    # )\n",
    "\n",
    "    if clf_name == \"LinearSVC\":\n",
    "        clf = LinearSVC(\n",
    "            dual=\"auto\",\n",
    "            random_state=0,\n",
    "            class_weight=\"balanced\",\n",
    "            C=trial.suggest_float(\"C\", 1e-5, 1e5, log=True),\n",
    "            penalty=trial.suggest_categorical(\"penalty\", [\"l1\", \"l2\"]),\n",
    "            max_iter=trial.suggest_int(\"max_iter\", 100, 10000),\n",
    "        )\n",
    "    elif clf_name == \"LogisticRegression\":\n",
    "        clf = LogisticRegression(\n",
    "            random_state=0,\n",
    "            class_weight=\"balanced\",\n",
    "            C=trial.suggest_float(\"C\", 1e-5, 1e5, log=True),\n",
    "        )\n",
    "    elif clf_name == \"DecisionTreeClassifier\":\n",
    "        clf = DecisionTreeClassifier(\n",
    "            random_state=0,\n",
    "            class_weight=\"balanced\",\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 1, 100),\n",
    "            criterion=trial.suggest_categorical(\n",
    "                \"criterion\", [\"gini\", \"entropy\", \"log_loss\"]\n",
    "            ),\n",
    "        )\n",
    "    elif clf_name == \"RandomForestClassifier\":\n",
    "        clf = RandomForestClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "            class_weight=\"balanced\",\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 1, 100),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 1, 100),\n",
    "            criterion=trial.suggest_categorical(\n",
    "                \"criterion\", [\"gini\", \"entropy\", \"log_loss\"]\n",
    "            ),\n",
    "            bootstrap=trial.suggest_categorical(\"bootstrap\", [True, False]),\n",
    "        )\n",
    "    elif clf_name == \"BalancedRandomForestClassifier\":\n",
    "        clf = BalancedRandomForestClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "            sampling_strategy=\"all\",\n",
    "            replacement=True,\n",
    "            bootstrap=False,\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 1, 100),\n",
    "            max_depth=trial.suggest_int(\"max_depth\", 1, 100),\n",
    "        )\n",
    "    elif clf_name == \"XGBClassifier\":\n",
    "        clf = XGBClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "            booster=trial.suggest_categorical(\n",
    "                \"booster\", [\"gbtree\", \"gblinear\", \"dart\"]\n",
    "            ),\n",
    "            alpha=trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            scale_pos_weight=2.8,\n",
    "        )\n",
    "    elif clf_name == \"LGBMClassifier\":\n",
    "        clf = LGBMClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state=0,\n",
    "            class_weight=\"balanced\",\n",
    "            boosting_type=trial.suggest_categorical(\n",
    "                \"boosting_type\", [\"gbdt\", \"dart\", \"goss\"]\n",
    "            ),\n",
    "            alpha=trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "            subsample=trial.suggest_float(\"subsample\", 0.2, 1),\n",
    "            colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1),\n",
    "            reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-8, 1.0, log=True),\n",
    "            reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        )\n",
    "    elif clf_name == \"DummyClassifier\":\n",
    "        clf = DummyClassifier(strategy=\"stratified\")\n",
    "    elif clf_name == \"IsolationForest\":\n",
    "        clf = IsolationForest(\n",
    "            random_state=0,\n",
    "            n_jobs=-1,\n",
    "            n_estimators=trial.suggest_int(\"n_estimators\", 1, 100),\n",
    "            max_samples=trial.suggest_int(\"max_samples\", 1, 100),\n",
    "            contamination=trial.suggest_float(\"contamination\", 0.01, 0.5),\n",
    "        )\n",
    "\n",
    "    # resampling_strategy = {\n",
    "    #     \"none\": None,\n",
    "    #     \"SMOTEENN\": SMOTEENN(random_state=0),\n",
    "    #     \"SMOTETomek\": SMOTETomek(random_state=0),\n",
    "    #     \"ClusterCentroids\": ClusterCentroids(\n",
    "    #         estimator=MiniBatchKMeans(random_state=0), random_state=0\n",
    "    #     ),\n",
    "    #     \"ADASYN\": ADASYN(random_state=0),\n",
    "    # }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "    scores = np.zeros(10)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        X_train_fold = preprocessing.fit_transform(X_train_fold)\n",
    "        X_test_fold = preprocessing.transform(X_test_fold)\n",
    "\n",
    "        # if resampling != \"none\":\n",
    "        #     X_train_fold, y_train_fold = resampling_strategy[resampling].fit_resample(\n",
    "        #         X_train_fold, y_train_fold\n",
    "        #     )\n",
    "\n",
    "        if clf_name == \"IsolationForest\":\n",
    "            clf.fit(X_train_fold[y_train_fold == 0])\n",
    "            y_pred = clf.predict(X_test_fold)\n",
    "            y_pred = np.where(y_pred == 1, 0, 1)\n",
    "        else:\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            y_pred = clf.predict(X_test_fold)\n",
    "\n",
    "        scores[i] = f1_score(y_test_fold, y_pred, zero_division=0)\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"clf_name\": clf_name,\n",
    "                # \"resampling\": resampling,\n",
    "                \"f1_score\": scores[i],\n",
    "                \"precision_score\": precision_score(\n",
    "                    y_test_fold, y_pred, zero_division=0\n",
    "                ),\n",
    "                \"recall_score\": recall_score(y_test_fold, y_pred, zero_division=0),\n",
    "                \"accuracy_score\": accuracy_score(y_test_fold, y_pred),\n",
    "                \"roc_auc_score\": roc_auc_score(y_test_fold, y_pred),\n",
    "                \"trial\": trial.number,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "clear_output()\n",
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best params\n",
    "# {'clf_name': 'LGBMClassifier',\n",
    "#  'boosting_type': 'dart',\n",
    "#  'alpha': 8.04643313873557e-05,\n",
    "#  'subsample': 0.5962446048419463,\n",
    "#  'colsample_bytree': 0.22677476520742115,\n",
    "#  'reg_alpha': 0.4230330447663563,\n",
    "#  'reg_lambda': 2.2652921293132893e-07}\n",
    "\n",
    "trials = study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_idx = trials.groupby(\"params_clf_name\")[\"value\"].idxmax()\n",
    "best_models = trials.loc[best_models_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_columns = [\n",
    "    \"number\",\n",
    "    \"state\",\n",
    "    \"datetime_start\",\n",
    "    \"datetime_complete\",\n",
    "    \"params_clf_name\",\n",
    "    \"duration\",\n",
    "    \"value\",\n",
    "]\n",
    "lgbm_params = (\n",
    "    best_models.loc[best_models[\"params_clf_name\"] == \"LGBMClassifier\"]\n",
    "    .dropna(axis=1)\n",
    "    .drop(columns=ignore_columns)\n",
    "    .iloc[0]\n",
    "    .rename(lambda x: x.replace(\"params_\", \"\"))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "linear_svc_params = (\n",
    "    best_models.loc[best_models[\"params_clf_name\"] == \"LinearSVC\"]\n",
    "    .dropna(axis=1)\n",
    "    .drop(columns=ignore_columns)\n",
    "    .iloc[0]\n",
    "    .rename(lambda x: x.replace(\"params_\", \"\"))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "lr_params = (\n",
    "    best_models.loc[best_models[\"params_clf_name\"] == \"LogisticRegression\"]\n",
    "    .dropna(axis=1)\n",
    "    .drop(columns=ignore_columns)\n",
    "    .iloc[0]\n",
    "    .rename(lambda x: x.replace(\"params_\", \"\"))\n",
    "    .to_dict()\n",
    ")\n",
    "linear_svc_params[\"max_iter\"] = int(linear_svc_params[\"max_iter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.boxplot(data=trials, x=\"params_clf_name\", y=\"value\")\n",
    "\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "# plt.ylim(0.5, 1.0)\n",
    "sns.despine()\n",
    "\n",
    "labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "plt.xticks(\n",
    "    ticks=np.arange(len(labels)),\n",
    "    labels=[\"\\n\".join(wrap(text, 18)) for text in labels],\n",
    ")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# ax = sns.boxplot(data=trials, x=\"params_resampling\", y=\"value\")\n",
    "\n",
    "# plt.xlabel(\"\")\n",
    "# plt.ylabel(\"\")\n",
    "\n",
    "# plt.ylim(0, 1.0)\n",
    "# sns.despine()\n",
    "\n",
    "# plt.tick_params(axis=\"x\", which=\"both\", bottom=False)\n",
    "\n",
    "# plt.xticks(\n",
    "#     ticks=[0, 1, 2, 3, 4],\n",
    "#     labels=[\n",
    "#         \"Sem reamostragem\" if (label := item.get_text()) == \"none\" else label\n",
    "#         for item in ax.get_xticklabels()\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    \"LinearSVC\": LinearSVC(\n",
    "        dual=\"auto\",\n",
    "        random_state=0,\n",
    "        class_weight=\"balanced\",\n",
    "    ),\n",
    "    \"LogisticRegression\": LogisticRegression(\n",
    "        random_state=0,\n",
    "        class_weight=\"balanced\",\n",
    "    ),\n",
    "    \"LGBMClassifier\": LGBMClassifier(\n",
    "        n_jobs=-1,\n",
    "        random_state=0,\n",
    "        class_weight=\"balanced\",\n",
    "        force_row_wise=True,\n",
    "    ),\n",
    "    \"DummyClassifier\": DummyClassifier(strategy=\"stratified\"),\n",
    "}\n",
    "\n",
    "\n",
    "metrics = []\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "for clf_name, clf in tqdm(clfs.items()):\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X_train, y_train)):\n",
    "        X_train_fold, X_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "        X_train_fold = preprocessing.fit_transform(X_train_fold)\n",
    "        X_test_fold = preprocessing.transform(X_test_fold)\n",
    "\n",
    "        clf.fit(X_train_fold, y_train_fold)\n",
    "        y_pred = clf.predict(X_test_fold)\n",
    "\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"fold\": i,\n",
    "                \"clf_name\": clf_name,\n",
    "                \"f1_score\": f1_score(y_test_fold, y_pred, zero_division=0),\n",
    "                \"precision_score\": precision_score(\n",
    "                    y_test_fold, y_pred, zero_division=0\n",
    "                ),\n",
    "                \"recall_score\": recall_score(y_test_fold, y_pred, zero_division=0),\n",
    "                \"accuracy_score\": accuracy_score(y_test_fold, y_pred),\n",
    "                \"roc_auc_score\": roc_auc_score(y_test_fold, y_pred),\n",
    "            }\n",
    "        )\n",
    "\n",
    "clear_output()\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "\n",
    "df_metrics.groupby(\"clf_name\").agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos possuem a pontuação de F1 muito próximas, por isso, para escolher o melhor modelo, será feito um teste de hipótese:\n",
    "\n",
    "- H0: Modelos tem a mesma performance no conjunto de dados\n",
    "- H1: Os modelos não possuem a mesma performance no conjunto de dados\n",
    "\n",
    "Nível de significância: 0.05\n",
    "\n",
    "Vamos conduzir o 5x2 cross-validation t-test para comparar os modelos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from mlxtend.evaluate import paired_ttest_5x2cv\n",
    "\n",
    "lgbm_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LGBMClassifier(\n",
    "                n_jobs=-1, random_state=0, class_weight=\"balanced\", **lgbm_params\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "logistic_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(random_state=0, class_weight=\"balanced\", **lr_params),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "svm_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LinearSVC(\n",
    "                dual=\"auto\",\n",
    "                random_state=0,\n",
    "                class_weight=\"balanced\",\n",
    "                **linear_svc_params,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "t, p = paired_ttest_5x2cv(\n",
    "    estimator1=lgbm_pipe,\n",
    "    estimator2=svm_pipe,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    scoring=\"f1\",\n",
    "    random_seed=0,\n",
    ")\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "print(f\"t-statistic: {t:.4f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\n",
    "        \"Já que o p-value é menor que 0.05, podemos rejeitar a hipótese nula e afirmar que um modelo é melhor que o outro\"\n",
    "    )\n",
    "else:\n",
    "    print(\n",
    "        \"Já que o p-value é maior que 0.05, não podemos rejeitar a hipótese nula e não podemos afirmar que um modelo é melhor que o outro\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos não possuem diferença significativa, mas todos superam o baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "best_lgbm_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LGBMClassifier(\n",
    "                n_jobs=-1, random_state=0, class_weight=\"balanced\", **lgbm_params\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "best_lr_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(random_state=0, class_weight=\"balanced\", **lr_params),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "best_svm_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LinearSVC(\n",
    "                dual=\"auto\",\n",
    "                random_state=0,\n",
    "                class_weight=\"balanced\",\n",
    "                **linear_svc_params,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "best_models = {\n",
    "    \"LGBMClassifier\": best_lgbm_pipe,\n",
    "    \"LogisticRegression\": best_lr_pipe,\n",
    "    \"LinearSVC\": best_svm_pipe,\n",
    "    \"DummyClassifier\": DummyClassifier(strategy=\"stratified\"),\n",
    "}\n",
    "\n",
    "\n",
    "best_lgbm_pipe.fit(X_train, y_train)\n",
    "best_lr_pipe.fit(X_train, y_train)\n",
    "best_svm_pipe.fit(X_train, y_train)\n",
    "best_models[\"DummyClassifier\"].fit(X_train, y_train)\n",
    "\n",
    "y_pred_lgbm = best_lgbm_pipe.predict(X_test)\n",
    "y_pred_lr = best_lr_pipe.predict(X_test)\n",
    "y_pred_svm = best_svm_pipe.predict(X_test)\n",
    "y_pred_dummy = best_models[\"DummyClassifier\"].predict(X_test)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(f\"{'LGBMClassifier':^30}\")\n",
    "print(classification_report(y_test, y_pred_lgbm, target_names=[\"Não Churn\", \"Churn\"]))\n",
    "\n",
    "print(f\"\\n\\n{'LogisticRegression':^30}\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=[\"Não Churn\", \"Churn\"]))\n",
    "\n",
    "print(f\"\\n\\n{'LinearSVC':^30}\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=[\"Não Churn\", \"Churn\"]))\n",
    "\n",
    "print(f\"\\n\\n{'DummyClassifier':^30}\")\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        best_models[\"DummyClassifier\"].predict(X_test),\n",
    "        target_names=[\"Não Churn\", \"Churn\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_test = []\n",
    "cms = []\n",
    "\n",
    "for clf_name, clf in best_models.items():\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    df_metrics_test.append(\n",
    "        {\n",
    "            \"classifier\": clf_name,\n",
    "            \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "            \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "            \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"roc_auc\": roc_auc_score(y_test, y_pred),\n",
    "        }\n",
    "    )\n",
    "    cms.append(pd.crosstab(y_test, y_pred, rownames=[\"Real\"], colnames=[\"Predito\"]))\n",
    "\n",
    "\n",
    "df_metrics_test = pd.DataFrame(df_metrics_test)\n",
    "df_metrics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    (\n",
    "        df_metrics_test.set_index(\"classifier\").mul(100).round(2).astype(str) + \"%\"\n",
    "    ).to_markdown()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, cm, clf_name in zip(axes.flatten(), cms, best_models.keys()):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        cbar=False,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(clf_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
